{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pacakge Initialisation for UNet in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as func\n",
    "from torch import Tensor\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from IPython.display import HTML, display\n",
    "from torchvision import transforms,datasets,utils\n",
    "%matplotlib inline\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_choices = {0:\"CityScape\", 1:\"PennFund\", 2:\"MNIST\"}\n",
    "dataset = 1\n",
    "\n",
    "batch_size = 5\n",
    "num_epochs = 15\n",
    "n_class = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cityscape_dataset(datadir_trainimage, datadir_testimage):\n",
    "    transform = transforms.Compose([transforms.ToTensor()]) # Converting to tensor dataset\n",
    "\n",
    "    train_set = torchvision.datasets.Cityscapes(root = datadir_trainimage,\n",
    "                                                split='train', mode='fine', target_type='semantic',\n",
    "                                                transform=transform , target_transform=transform , transforms=None)\n",
    "    test_set = torchvision.datasets.Cityscapes(root = datadir_testimage,\n",
    "                                               split='val', mode='fine', target_type='semantic',\n",
    "                                               transform=transform , target_transform=transform , transforms=None)\n",
    "\n",
    "\n",
    "    # creating data loader for train and test\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "        'test': DataLoader(test_set, batch_size=1, shuffle=True, num_workers=2)\n",
    "    }\n",
    "    \n",
    "    return dataloaders\n",
    "    \n",
    "def laod_pennfund_dataset(datadir_trainimage, datadir_testimage):\n",
    "\n",
    "    \n",
    "    def process_images_and_masks(datadir_trainimage, datadir_testimage):\n",
    "        # training images\n",
    "        trainingdata=[] #empty array for saving training images as numpy array\n",
    "        trainingfile=[] #empty array for saving  corresponding training file names as numpy array\n",
    "        for img in os.listdir(datadir_trainimage):\n",
    "            trainingfile.append(img)\n",
    "            img_array =cv2.imread(os.path.join(datadir_trainimage,img),cv2.COLOR_RGB2GRAY)\n",
    "            new_array = cv2.resize(img_array,(400,400))   \n",
    "            trainingdata.append(np.squeeze([new_array]))\n",
    "\n",
    "        # masking images    \n",
    "        maskingdata=[] #empty array for saving target mask images as numpy array\n",
    "        maskingfile = [] #empty array for saving  corresponding target mask file names as numpy array\n",
    "        for i in range(len(trainingfile)):\n",
    "            img = (trainingfile[i].rsplit('.png', 1)[0])+str(\"_mask.png\")\n",
    "            maskingfile.append(img)\n",
    "            img_array =cv2.imread(os.path.join(datadir_testimage,img),cv2.IMREAD_GRAYSCALE)\n",
    "            new_array = cv2.resize(img_array,(400,400))\n",
    "            new_array[np.where(new_array>0.5)] = 1\n",
    "            maskingdata.append(np.squeeze([new_array]))\n",
    "\n",
    "        #Converting into numpy which is easy to compute\n",
    "        trainingdata = np.array(trainingdata)\n",
    "        maskingdata = np.array(maskingdata)\n",
    "        \n",
    "        return trainingdata, maskingdata\n",
    "\n",
    "    def test_mask_plots(trainingdata, maskingdata):\n",
    "        plt.imshow(maskingdata[14])\n",
    "        plt.show()\n",
    "        print(\"Unique pixels in Labels: \" + str(np.unique(maskingdata[7])))\n",
    "        print(\"Shape of Mask Labels: \" + str(maskingdata[7].shape))\n",
    "        print(\"Shape of Input Images: \" + str(trainingdata[7].shape))\n",
    "\n",
    "    \n",
    "    # Pre-process dataset\n",
    "    trainingdata, maskingdata = process_images_and_masks(datadir_trainimage, datadir_testimage)\n",
    "    # Test Masks\n",
    "    test_mask_plots(trainingdata, maskingdata)\n",
    "    \n",
    "        \n",
    "    # Final Dataset Creator\n",
    "    # RGB images obtained from CV2 has channel different than expected tensor\n",
    "    a = Tensor(trainingdata)\n",
    "    trainingdata_tensor = torch.empty(170,3,400,400,dtype=torch.float)\n",
    "    for i, img in enumerate(a):\n",
    "        trainingdata_tensor[i,:,:,:] = img.permute(2,0,1)\n",
    "\n",
    "    # Converting to tensor dataset\n",
    "    dataset_created = TensorDataset(trainingdata_tensor, Tensor(maskingdata))\n",
    "    torch.manual_seed(0)\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset_created, [10, 160])\n",
    "\n",
    "    # creating data loader for train and test\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "        'test': DataLoader(val_set, batch_size=1, shuffle=True, num_workers=2)\n",
    "        }\n",
    "    \n",
    "    return dataloaders\n",
    "\n",
    "def label_split(labels,targetimage,n_class):\n",
    "    for i in range(n_class):\n",
    "        if i==0:\n",
    "            labels[:,0,:,:] = (targetimage == 0)==1\n",
    "        else:\n",
    "            labels[:,i,:,:] = (targetimage != i)==0\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation based on new data image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique pixels in Labels: [0 1]\n",
      "Shape of Mask Labels: (400, 400)\n",
      "Shape of Input Images: (400, 400, 3)\n"
     ]
    }
   ],
   "source": [
    "if dataset==0:\n",
    "    datadir_trainimage = \"../Dataset/cityscape_400x400/\"\n",
    "    datadir_testimage  = \"../Dataset/cityscape_400x400/\"\n",
    "    dataloaders = load_cityscape_dataset(datadir_trainimage, datadir_testimage)\n",
    "elif dataset==1:\n",
    "    datadir_trainimage = \"PennFudanPed/PNGImages\"\n",
    "    datadir_testimage  = \"PennFudanPed/PedMasks\"\n",
    "    dataloaders = laod_pennfund_dataset(datadir_trainimage, datadir_testimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNET Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16k Params Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 16, 400, 400]        --\n",
      "|    └─Conv2d: 2-1                       [-1, 16, 400, 400]        448\n",
      "|    └─ReLU: 2-2                         [-1, 16, 400, 400]        --\n",
      "├─MaxPool2d: 1-2                         [-1, 16, 200, 200]        --\n",
      "├─Sequential: 1-3                        [-1, 32, 200, 200]        --\n",
      "|    └─Conv2d: 2-3                       [-1, 32, 200, 200]        4,640\n",
      "|    └─ReLU: 2-4                         [-1, 32, 200, 200]        --\n",
      "├─ConvTranspose2d: 1-4                   [-1, 32, 400, 400]        4,128\n",
      "├─Sequential: 1-5                        [-1, 16, 400, 400]        --\n",
      "|    └─Conv2d: 2-5                       [-1, 16, 400, 400]        6,928\n",
      "|    └─ReLU: 2-6                         [-1, 16, 400, 400]        --\n",
      "├─Conv2d: 1-6                            [-1, 2, 400, 400]         34\n",
      "==========================================================================================\n",
      "Total params: 16,178\n",
      "Trainable params: 16,178\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 2.02\n",
      "==========================================================================================\n",
      "Input size (MB): 1.83\n",
      "Forward/backward pass size (MB): 90.33\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 92.22\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "class unet_16k(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(unet_16k,self).__init__()\n",
    "\n",
    "        #Encoder\n",
    "        self.contractinglayer1 = nn.Sequential(nn.Conv2d(3, 16, 3, stride=1, padding=1), nn.ReLU())\n",
    "        self.contractinglayer2 = nn.Sequential(nn.Conv2d(16, 32, 3, stride=1, padding=1), nn.ReLU())\n",
    "        self.downsampling = nn.MaxPool2d(2)\n",
    "\n",
    "        #Decoder\n",
    "        self.upsampling_2to1 = nn.ConvTranspose2d(32, 32, 2,stride=2, padding=0)\n",
    "        self.explayer5 = nn.Sequential(nn.Conv2d(32+16, 16, 3, stride=1, padding=1), nn.ReLU())\n",
    "        self.output_conv = nn.Conv2d(16, 2, 1, stride=1)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #Encoder\n",
    "        x1 = self.contractinglayer1(x)\n",
    "        x2 = self.downsampling(x1)\n",
    "        x3 = self.contractinglayer2(x2)\n",
    "        \n",
    "        #Decoder\n",
    "        x4 = torch.cat([self.upsampling_2to1(x3,output_size=x1.size()), x1], dim=1)\n",
    "        x5 = self.explayer5(x4)\n",
    "        x6 = self.output_conv(x5)\n",
    "        \n",
    "        return x6   \n",
    "\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "model = unet_16k()\n",
    "model = model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "_ = summary(model, (3, 400, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100k Params Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 16, 400, 400]        --\n",
      "|    └─Conv2d: 2-1                       [-1, 16, 400, 400]        448\n",
      "|    └─ReLU: 2-2                         [-1, 16, 400, 400]        --\n",
      "|    └─Conv2d: 2-3                       [-1, 16, 400, 400]        2,320\n",
      "|    └─ReLU: 2-4                         [-1, 16, 400, 400]        --\n",
      "├─MaxPool2d: 1-2                         [-1, 16, 200, 200]        --\n",
      "├─Sequential: 1-3                        [-1, 32, 200, 200]        --\n",
      "|    └─Conv2d: 2-5                       [-1, 32, 200, 200]        4,640\n",
      "|    └─ReLU: 2-6                         [-1, 32, 200, 200]        --\n",
      "|    └─Conv2d: 2-7                       [-1, 32, 200, 200]        9,248\n",
      "|    └─ReLU: 2-8                         [-1, 32, 200, 200]        --\n",
      "├─MaxPool2d: 1-4                         [-1, 32, 100, 100]        --\n",
      "├─Sequential: 1-5                        [-1, 64, 100, 100]        --\n",
      "|    └─Conv2d: 2-9                       [-1, 64, 100, 100]        18,496\n",
      "|    └─ReLU: 2-10                        [-1, 64, 100, 100]        --\n",
      "|    └─Conv2d: 2-11                      [-1, 64, 100, 100]        36,928\n",
      "|    └─ReLU: 2-12                        [-1, 64, 100, 100]        --\n",
      "├─ConvTranspose2d: 1-6                   [-1, 64, 200, 200]        16,448\n",
      "├─Sequential: 1-7                        [-1, 32, 200, 200]        --\n",
      "|    └─Conv2d: 2-13                      [-1, 32, 200, 200]        27,680\n",
      "|    └─ReLU: 2-14                        [-1, 32, 200, 200]        --\n",
      "|    └─Conv2d: 2-15                      [-1, 32, 200, 200]        9,248\n",
      "|    └─ReLU: 2-16                        [-1, 32, 200, 200]        --\n",
      "├─ConvTranspose2d: 1-8                   [-1, 32, 400, 400]        4,128\n",
      "├─Sequential: 1-9                        [-1, 16, 400, 400]        --\n",
      "|    └─Conv2d: 2-17                      [-1, 16, 400, 400]        6,928\n",
      "|    └─ReLU: 2-18                        [-1, 16, 400, 400]        --\n",
      "|    └─Conv2d: 2-19                      [-1, 16, 400, 400]        2,320\n",
      "|    └─ReLU: 2-20                        [-1, 16, 400, 400]        --\n",
      "├─Conv2d: 1-10                           [-1, 2, 400, 400]         34\n",
      "==========================================================================================\n",
      "Total params: 138,866\n",
      "Trainable params: 138,866\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 5.81\n",
      "==========================================================================================\n",
      "Input size (MB): 1.83\n",
      "Forward/backward pass size (MB): 187.99\n",
      "Params size (MB): 0.53\n",
      "Estimated Total Size (MB): 190.35\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "class unet_100k(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(unet_100k,self).__init__()\n",
    "\n",
    "        #Encoder\n",
    "        self.contractinglayer1 = nn.Sequential(nn.Conv2d(3, 16, 3, stride=1, padding=1), nn.ReLU(), nn.Conv2d(16, 16, 3, stride=1, padding=1), nn.ReLU())\n",
    "        self.contractinglayer2 = nn.Sequential(nn.Conv2d(16, 32, 3, stride=1, padding=1), nn.ReLU(), nn.Conv2d(32, 32, 3, stride=1, padding=1), nn.ReLU())\n",
    "        self.contractinglayer3 = nn.Sequential(nn.Conv2d(32, 64, 3, stride=1, padding=1), nn.ReLU(), nn.Conv2d(64, 64, 3, stride=1, padding=1), nn.ReLU())\n",
    "        self.downsampling = nn.MaxPool2d(2)\n",
    "\n",
    "        #Decoder\n",
    "        self.upsampling_3to2 = nn.ConvTranspose2d(64, 64, 2,stride=2, padding=0)\n",
    "        self.explayer4 = nn.Sequential(nn.Conv2d(64+32, 32, 3, stride=1, padding=1), nn.ReLU(), nn.Conv2d(32, 32, 3, stride=1, padding=1), nn.ReLU())\n",
    "        self.upsampling_2to1 = nn.ConvTranspose2d(32, 32, 2,stride=2, padding=0)\n",
    "        self.explayer5 = nn.Sequential(nn.Conv2d(32+16, 16, 3, stride=1, padding=1), nn.ReLU(), nn.Conv2d(16, 16, 3, stride=1, padding=1), nn.ReLU())\n",
    "        self.output_conv = nn.Conv2d(16, 2, 1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Encoder\n",
    "        x1 = self.contractinglayer1(x)\n",
    "        x2 = self.downsampling(x1)\n",
    "        x3 = self.contractinglayer2(x2)\n",
    "        x4 = self.downsampling(x3)\n",
    "        x5 = self.contractinglayer3(x4)\n",
    "        \n",
    "        #Decoder\n",
    "        x6 = torch.cat([self.upsampling_3to2(x5,output_size=x3.size()), x3], dim=1)\n",
    "        x7 = self.explayer4(x6)\n",
    "        x8 = torch.cat([self.upsampling_2to1(x7,output_size=x1.size()), x1], dim=1)\n",
    "        x9 = self.explayer5(x8)\n",
    "        x10 = self.output_conv(x9)\n",
    "        \n",
    "        return x10   \n",
    "\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = unet_100k()\n",
    "model = model.to(device)\n",
    "\n",
    "_ = summary(model, (3, 400, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7M Params Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 64, 400, 400]        --\n",
      "|    └─Conv2d: 2-1                       [-1, 64, 400, 400]        1,792\n",
      "|    └─ReLU: 2-2                         [-1, 64, 400, 400]        --\n",
      "|    └─Conv2d: 2-3                       [-1, 64, 400, 400]        36,928\n",
      "|    └─ReLU: 2-4                         [-1, 64, 400, 400]        --\n",
      "├─MaxPool2d: 1-2                         [-1, 64, 200, 200]        --\n",
      "├─Sequential: 1-3                        [-1, 128, 200, 200]       --\n",
      "|    └─Conv2d: 2-5                       [-1, 128, 200, 200]       73,856\n",
      "|    └─ReLU: 2-6                         [-1, 128, 200, 200]       --\n",
      "|    └─Conv2d: 2-7                       [-1, 128, 200, 200]       147,584\n",
      "|    └─ReLU: 2-8                         [-1, 128, 200, 200]       --\n",
      "├─MaxPool2d: 1-4                         [-1, 128, 100, 100]       --\n",
      "├─Sequential: 1-5                        [-1, 256, 100, 100]       --\n",
      "|    └─Conv2d: 2-9                       [-1, 256, 100, 100]       295,168\n",
      "|    └─ReLU: 2-10                        [-1, 256, 100, 100]       --\n",
      "|    └─Conv2d: 2-11                      [-1, 256, 100, 100]       590,080\n",
      "|    └─ReLU: 2-12                        [-1, 256, 100, 100]       --\n",
      "├─MaxPool2d: 1-6                         [-1, 256, 50, 50]         --\n",
      "├─Sequential: 1-7                        [-1, 512, 50, 50]         --\n",
      "|    └─Conv2d: 2-13                      [-1, 512, 50, 50]         1,180,160\n",
      "|    └─ReLU: 2-14                        [-1, 512, 50, 50]         --\n",
      "|    └─Conv2d: 2-15                      [-1, 512, 50, 50]         2,359,808\n",
      "|    └─ReLU: 2-16                        [-1, 512, 50, 50]         --\n",
      "├─Upsample: 1-8                          [-1, 512, 100, 100]       --\n",
      "├─Sequential: 1-9                        [-1, 256, 100, 100]       --\n",
      "|    └─Conv2d: 2-17                      [-1, 256, 100, 100]       1,769,728\n",
      "|    └─ReLU: 2-18                        [-1, 256, 100, 100]       --\n",
      "|    └─Conv2d: 2-19                      [-1, 256, 100, 100]       590,080\n",
      "|    └─ReLU: 2-20                        [-1, 256, 100, 100]       --\n",
      "├─Upsample: 1-10                         [-1, 256, 200, 200]       --\n",
      "├─Sequential: 1-11                       [-1, 128, 200, 200]       --\n",
      "|    └─Conv2d: 2-21                      [-1, 128, 200, 200]       442,496\n",
      "|    └─ReLU: 2-22                        [-1, 128, 200, 200]       --\n",
      "|    └─Conv2d: 2-23                      [-1, 128, 200, 200]       147,584\n",
      "|    └─ReLU: 2-24                        [-1, 128, 200, 200]       --\n",
      "├─Upsample: 1-12                         [-1, 128, 400, 400]       --\n",
      "├─Sequential: 1-13                       [-1, 64, 400, 400]        --\n",
      "|    └─Conv2d: 2-25                      [-1, 64, 400, 400]        110,656\n",
      "|    └─ReLU: 2-26                        [-1, 64, 400, 400]        --\n",
      "|    └─Conv2d: 2-27                      [-1, 64, 400, 400]        36,928\n",
      "|    └─ReLU: 2-28                        [-1, 64, 400, 400]        --\n",
      "├─Conv2d: 1-14                           [-1, 2, 400, 400]         130\n",
      "==========================================================================================\n",
      "Total params: 7,782,978\n",
      "Trainable params: 7,782,978\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 103.52\n",
      "==========================================================================================\n",
      "Input size (MB): 1.83\n",
      "Forward/backward pass size (MB): 568.85\n",
      "Params size (MB): 29.69\n",
      "Estimated Total Size (MB): 600.37\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )   \n",
    "\n",
    "\n",
    "class UNet_7M(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "              \n",
    "        #Encoder\n",
    "        self.dconv_down1 = double_conv(3, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dconv_down4 = double_conv(256, 512)        \n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2,stride=2)\n",
    "        \n",
    "        #Decoder\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, 2, 1,stride=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Encoder\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)   \n",
    "        x = self.dconv_down4(x)\n",
    "        \n",
    "        #Decoder\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv2], dim=1)       \n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv1], dim=1)   \n",
    "        x = self.dconv_up1(x)\n",
    "        out = self.conv_last(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet_7M()\n",
    "model = model.to(device)\n",
    "\n",
    "_ = summary(model, (3, 400, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice Loss + BCE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    \n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "from collections import defaultdict\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "def calc_loss(pred, target, bce_weight=0.5):\n",
    "    bce = func.binary_cross_entropy_with_logits(pred, target).type(dtype)\n",
    "\n",
    "    pred = torch.sigmoid(pred).type(dtype)\n",
    "    dice = dice_loss(pred, target)\n",
    "\n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def iou(pred, target, smooth=1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    union = pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) \n",
    "    #print(intersection,union)\n",
    "   \n",
    "    loss = ((intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) - intersection + smooth))\n",
    "    \n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture choosen with parameters: 7782978\n"
     ]
    }
   ],
   "source": [
    "# Which UNet Model?\n",
    "unet = UNet_7M()\n",
    "\n",
    "print(\"Architecture choosen with parameters: \" + str(sum(p.numel() for p in unet.parameters())))\n",
    "gpu_available = torch.cuda.is_available() \n",
    "if gpu_available:\n",
    "    unet = unet.cuda()\n",
    "    \n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, unet.parameters()), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "----------\n",
      "LR 0.0001\n",
      "Epoch 1/14\n",
      "----------\n",
      "LR 0.0001\n",
      "Epoch 2/14\n",
      "----------\n",
      "LR 0.0001\n",
      "Epoch 3/14\n",
      "----------\n",
      "LR 0.0001\n",
      "Epoch 4/14\n",
      "----------\n",
      "LR 0.0001\n"
     ]
    }
   ],
   "source": [
    "loss_values =[]\n",
    "running_loss = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "\n",
    "    scheduler.step()\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(\"LR\", param_group['lr'])\n",
    "\n",
    "    for i, data in enumerate(dataloaders['train']):\n",
    "        inputimage, targetimage = data\n",
    "        \n",
    "        if gpu_available:\n",
    "            inputimage = inputimage.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward Pass \n",
    "        outputs = unet(inputimage)\n",
    "\n",
    "        # Loss Calculation\n",
    "        labels = torch.empty_like(outputs)\n",
    "        lab_channel = (label_split(labels,targetimage,n_class))\n",
    "        loss = calc_loss(outputs,lab_channel)\n",
    "\n",
    "        # Backward Pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 10 == 9:\n",
    "            print('[%d, %5d] loss: %.10f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "            loss_values.append(running_loss / 170)\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unet.state_dict(), os.path.join(\"PennFudanPed/\", UNet_7M()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_threshold = 0.6 # enter the threshold for IOU metrics\n",
    "correct_prediction = 0\n",
    "total_prediction = 0\n",
    "for i, data in enumerate(dataloaders['test']):\n",
    "    inputimage, targetimage = data\n",
    "    \n",
    "    if gpu_available:\n",
    "        inputimage = inputimage.cuda() \n",
    "        \n",
    "\n",
    "    prediction = unet(inputimage)\n",
    "    prediction = torch.sigmoid(prediction)  \n",
    "    \n",
    "    labels = torch.empty_like(prediction)\n",
    "    lab_channel = (label_split(labels,targetimage,n_class)).cuda()\n",
    "\n",
    "    loss_raw = ((iou(prediction,lab_channel,1e-9)).cpu())\n",
    "    \n",
    "    total_prediction = total_prediction + 1\n",
    "    if loss_raw > iou_threshold:\n",
    "      correct_prediction = correct_prediction + 1\n",
    "\n",
    "print(\"IOU loss: \" + str(correct_prediction/total_prediction) +str(\" for IOU threshold of: \") + str(iou_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Get the first batch\n",
    "inputs1, targetimage = next(iter(dataloaders['test']))\n",
    "inputs = inputs1.cuda()\n",
    "\n",
    "# Predict\n",
    "pred = unet(inputs)\n",
    "\n",
    "# The loss functions include the sigmoid function.\n",
    "pred = torch.sigmoid(pred)\n",
    "pred_max = torch.argmax(pred, dim=1)\n",
    "\n",
    "\n",
    "#printing image\n",
    "fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(12,12),facecolor='w')\n",
    "ax1.imshow(pred_max.squeeze(0).cpu(),  interpolation='none',cmap='jet')\n",
    "ax2.imshow(targetimage.squeeze(0).cpu(),  interpolation='none',cmap='jet')\n",
    "ax3.imshow(inputs1[:,2,:,:].squeeze(0).cpu(),  interpolation='none',cmap='gray')\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "ax3.axis('off')\n",
    "ax1.set_title('prediction',fontsize=20)\n",
    "ax2.set_title('Label',fontsize=20)\n",
    "ax3.set_title('input',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
